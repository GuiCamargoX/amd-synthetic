{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files under the directory\n",
    "\n",
    "path=os.getcwd()+ r'\\raw\\ODIR-5K\\ODIR-5K_Training_Dataset'\n",
    "\n",
    "image_folder=[]\n",
    "for dirname, _, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        image_folder.append( filename )\n",
    "\n",
    "df = pd.read_excel(\"raw\\ODIR-5K\\ODIR-5K_Training_Annotations_V2.xlsx\", index_col='ID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h3> Extracting AMD & Normal information from the Dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df[\"left_amd\"] = df[\"Left-Diagnostic Keywords\"].apply(lambda x: 1 if \"age-related\" in x.lower() else 0)\n",
    "        #df[\"left_drywet\"] = df[\"Left-Diagnostic Keywords\"].apply(lambda x: 1 if \"wet age-related\" in x.lower() else 0)\n",
    "\n",
    "    df[\"right_amd\"] = df[\"Right-Diagnostic Keywords\"].apply(lambda x: 1 if \"age-related\" in x.lower() else 0)\n",
    "        #df[\"right_drywet\"] = df[\"Right-Diagnostic Keywords\"].apply(lambda x: 1 if \"wet age-related\" in x.lower() else 0)\n",
    "        \n",
    "    df_left = df[[ \"Left-Fundus\", \"left_amd\"] ].rename({\"Left-Fundus\":'Fundus', \"left_amd\":'AMD'},axis=1)\n",
    "    df_right = df[[ \"Right-Fundus\", \"right_amd\"] ].rename({\"Right-Fundus\":'Fundus', \"right_amd\":'AMD' },axis=1)\n",
    "        \n",
    "    return pd.concat([df_left, df_right])\n",
    "\n",
    "df = preprocess(df)\n",
    "df['Fundus'] = df['Fundus'].apply(lambda x: x.split('.')[0]+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> <h3> Split Train/Test </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_aux, df_test = train_test_split(df, test_size=0.20, random_state=42)\n",
    "df_train, df_valid = train_test_split(df_aux, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h3> Structure the dataset in order to use pytorch Image Folder class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import os.path\n",
    "\n",
    "\n",
    "def arrange_data(df, dataset_dir):\n",
    "       \n",
    "    amd = df.loc[df[\"AMD\"] == 1, \"Fundus\"].values\n",
    "    normal = df.loc[df[\"AMD\"] == 0, \"Fundus\"].values\n",
    "    \n",
    "    #creating dir\n",
    "    amd_path= dataset_dir+r'\\amd'\n",
    "    normal_path= dataset_dir+r'\\normal'\n",
    "    orig_path= os.getcwd()+ r'\\raw\\ODIR-5K\\ODIR-5K_Training_Dataset'\n",
    "    \n",
    "\n",
    "    try:\n",
    "        os.mkdir(amd_path)\n",
    "        os.mkdir(normal_path)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "    \n",
    "    #print(image_folder)\n",
    "    #print(\"transfer AMD files from ODIR \")\n",
    "    for img in tqdm(amd):\n",
    "        original_image_path = os.path.join(orig_path,img)\n",
    "        if img in image_folder:\n",
    "            shutil.copy2(original_image_path, amd_path)\n",
    "\n",
    "    #print(\"transfer Normal files from ODIR \")\n",
    "    for img in tqdm(normal):\n",
    "        original_image_path = os.path.join(orig_path,img)\n",
    "        if img in image_folder:\n",
    "            shutil.copy2(original_image_path, normal_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RIADD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files under the directory\n",
    "\n",
    "path=os.getcwd()+ r'\\raw\\RIADD\\Training_set\\Training'\n",
    "\n",
    "image_folder=[]\n",
    "for dirname, _, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        image_folder.append( filename )\n",
    "\n",
    "df = pd.read_csv(\"raw\\RIADD\\Training_set\\RFMiD_Training_Labels.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import os.path\n",
    "\n",
    "\n",
    "def arrange_data(df, dataset_dir):\n",
    "       \n",
    "    amd = df.loc[df[\"ARMD\"] == 1, \"ID\"].values\n",
    "    normal = df.loc[df[\"ARMD\"] == 0, \"ID\"].values\n",
    "    \n",
    "    #creating dir\n",
    "    amd_path= dataset_dir+r'\\amd'\n",
    "    normal_path= dataset_dir+r'\\normal'\n",
    "    orig_path= os.getcwd()+ r'\\raw\\RIADD\\Training_set\\Training'\n",
    "    \n",
    "\n",
    "    try:\n",
    "        os.mkdir(amd_path)\n",
    "        os.mkdir(normal_path)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "    \n",
    "    #print(image_folder)\n",
    "    #print(\"transfer AMD files from ODIR \")\n",
    "    for img in tqdm(amd):\n",
    "        name_img= str(img)+'.png'\n",
    "        original_image_path = os.path.join(orig_path,name_img)\n",
    "        #print(original_image_path)\n",
    "        if name_img in image_folder:\n",
    "            #print(original_image_path)\n",
    "            shutil.copy2(original_image_path, amd_path)\n",
    "\n",
    "    #print(\"transfer Normal files from ODIR \")\n",
    "    for img in tqdm(normal):\n",
    "        name_img= str(img)+'.png'\n",
    "        original_image_path = os.path.join(orig_path,name_img)\n",
    "        if name_img in image_folder:\n",
    "            shutil.copy2(original_image_path, normal_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 213.12it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1820/1820 [00:08<00:00, 210.45it/s]\n"
     ]
    }
   ],
   "source": [
    "path= os.getcwd()+ r'\\preprocess_1_folder\\RIADD'\n",
    "\n",
    "arrange_data(df, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# folder to resize_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eyeq_preprocess import fundus_prep as prep\n",
    "import glob\n",
    "import os\n",
    "import cv2 as cv\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(image_list, save_path):\n",
    "    \n",
    "    for image_path in image_list:\n",
    "        dst_image = os.path.splitext(image_path.split(\"\\\\\")[-1])[0]+'.png'\n",
    "        dst_path = os.path.join(save_path, dst_image)\n",
    "        if os.path.exists(dst_path):\n",
    "            print('continue...')\n",
    "            continue\n",
    "        try:\n",
    "            img = prep.imread(image_path)\n",
    "            r_img, borders, mask = prep.process_without_gb(img)\n",
    "            r_img = cv.resize(r_img, (800, 800))\n",
    "            prep.imwrite(dst_path, r_img)\n",
    "            # mask = cv.resize(mask, (800, 800))\n",
    "            # prep.imwrite(os.path.join('./original_mask', dst_image), mask)\n",
    "        except:\n",
    "            print(image_path)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ODIR-5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.getcwd()+ r'\\preprocess_2_crop_resize_align\\ODIR-5K'\n",
    "path_amd = path+r'\\AMD'\n",
    "path_non = path+r'\\Non-AMD'\n",
    "\n",
    "\n",
    "path_orig= os.getcwd()+ r'\\preprocess_1_folder\\ODIR-5K'\n",
    "path_orig_amd = path_orig+r'\\AMD'\n",
    "path_orig_non = path_orig+r'\\Non-AMD'\n",
    "\n",
    "image_list = glob.glob(os.path.join(path_orig_amd, '*.jpg'))\n",
    "save_path = prep.fold_dir(path_amd)\n",
    "process(image_list, save_path)\n",
    "\n",
    "image_list = glob.glob(os.path.join(path_orig_non, '*.jpg'))\n",
    "save_path = prep.fold_dir(path_non)\n",
    "process(image_list, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baidu_Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.getcwd()+ r'\\preprocess_2_crop_resize_align\\Baidu_Challenge'\n",
    "path_amd = path+r'\\AMD'\n",
    "path_non = path+r'\\Non-AMD'\n",
    "\n",
    "\n",
    "path_orig= os.getcwd()+ r'\\preprocess_1_folder\\Baidu_Challenge'\n",
    "path_orig_amd = path_orig+r'\\AMD'\n",
    "path_orig_non = path_orig+r'\\Non-AMD'\n",
    "\n",
    "image_list = glob.glob(os.path.join(path_orig_amd, '*.jpg'))\n",
    "save_path = prep.fold_dir(path_amd)\n",
    "process(image_list, save_path)\n",
    "\n",
    "image_list = glob.glob(os.path.join(path_orig_non, '*.jpg'))\n",
    "save_path = prep.fold_dir(path_non)\n",
    "process(image_list, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RIADD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.getcwd()+ r'\\preprocess_2_crop_resize_align\\RIADD'\n",
    "path_amd = path+r'\\AMD'\n",
    "path_non = path+r'\\Non-AMD'\n",
    "\n",
    "\n",
    "path_orig= os.getcwd()+ r'\\preprocess_1_folder\\RIADD'\n",
    "path_orig_amd = path_orig+r'\\AMD'\n",
    "path_orig_non = path_orig+r'\\Non-AMD'\n",
    "\n",
    "image_list = glob.glob(os.path.join(path_orig_amd, '*.png'))\n",
    "save_path = prep.fold_dir(path_amd)\n",
    "process(image_list, save_path)\n",
    "\n",
    "image_list = glob.glob(os.path.join(path_orig_non, '*.png'))\n",
    "save_path = prep.fold_dir(path_non)\n",
    "process(image_list, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resize_align to Quality assesment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import time\n",
    "from utils.eyeq_model.progress.bar import Bar\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from utils.eyeq_model.dataloader.EyeQ_loader import DatasetGenerator\n",
    "\n",
    "import pandas as pd\n",
    "from utils.eyeq_model.networks.densenet_mcf import dense121_mcs\n",
    "import shutil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prop_quality(test_images_dir, path_dest ):    \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    #data_root = '../Kaggle_DR_dataset/'\n",
    "\n",
    "    # Setting parameters\n",
    "    parser = argparse.ArgumentParser(description='EyeQ_dense121')\n",
    "    parser.add_argument('--model_dir', type=str, default='./utils/eyeq_model/result')\n",
    "    parser.add_argument('--pre_model', type=str, default='DenseNet121_v3_v1')\n",
    "    #parser.add_argument('--save_model', type=str, default='DenseNet121_v3_v1')\n",
    "\n",
    "    parser.add_argument('--crop_size', type=int, default=224)\n",
    "    parser.add_argument('--label_idx', type=list, default=['Good', 'Usable', 'Reject'])\n",
    "\n",
    "    parser.add_argument('--n_classes', type=int, default=3)\n",
    "    # Optimization options\n",
    "    parser.add_argument('--epochs', default=20, type=int)\n",
    "    parser.add_argument('--batch-size', default=2, type=int)\n",
    "    parser.add_argument('--lr', default=0.01, type=float)\n",
    "    parser.add_argument('--loss_w', default=[0.1, 0.1, 0.1, 0.1, 0.6], type=list)\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "\n",
    "    # options\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    model = dense121_mcs(n_class=args.n_classes)\n",
    "\n",
    "    if args.pre_model is not None:\n",
    "        loaded_model = torch.load(os.path.join(args.model_dir, args.pre_model + '.tar'))\n",
    "        model.load_state_dict(loaded_model['state_dict'])\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = torch.nn.BCELoss(reduction='mean')\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "    print('Total params: %.2fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
    "\n",
    "\n",
    "    transform_list_val1 = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "        ])\n",
    "\n",
    "    transformList2 = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "\n",
    "    data_test = DatasetGenerator(data_dir=test_images_dir,list_file=None, transform1=transform_list_val1,\n",
    "                                transform2=transformList2, n_class=args.n_classes, set_name='test')\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=data_test, batch_size=args.batch_size,\n",
    "                                            shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "    # Testing\n",
    "    outPRED_mcs = torch.FloatTensor().cuda()\n",
    "    img_name_mcs = []\n",
    "    model.eval()\n",
    "    iters_per_epoch = len(test_loader)\n",
    "    bar = Bar('Processing {}'.format('inference'), max=len(test_loader))\n",
    "    bar.check_tty = False\n",
    "    for epochID, (imagesA, imagesB, imagesC, image_name) in enumerate(test_loader):\n",
    "        imagesA = imagesA.cuda()\n",
    "        imagesB = imagesB.cuda()\n",
    "        imagesC = imagesC.cuda()\n",
    "\n",
    "        begin_time = time.time()\n",
    "        _, _, _, _, result_mcs = model(imagesA, imagesB, imagesC)\n",
    "        outPRED_mcs = torch.cat((outPRED_mcs, result_mcs.data), 0)\n",
    "        for i in image_name:\n",
    "            img_name_mcs.append(i)\n",
    "\n",
    "        batch_time = time.time() - begin_time\n",
    "        bar.suffix = '{} / {} | Time: {batch_time:.4f}'.format(epochID + 1, len(test_loader),\n",
    "                                                            batch_time=batch_time * (iters_per_epoch - epochID) / 60)\n",
    "        bar.next()\n",
    "    bar.finish()\n",
    "\n",
    "    pred= torch.argmax(outPRED_mcs,1).cpu().numpy()\n",
    "    img_name_mcs= np.asarray(img_name_mcs)\n",
    "    print(outPRED_mcs)\n",
    "\n",
    "    path=path_dest\n",
    "\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "\n",
    "    goods = img_name_mcs[pred!=2]\n",
    "    for img in tqdm( goods ):\n",
    "        original_image_path = img\n",
    "        shutil.copy2(original_image_path, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baidu_Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_dir = os.getcwd()+ r'\\preprocess_2_crop_resize_align\\Baidu_Challenge'\n",
    "path_amd = test_images_dir+r'\\AMD'\n",
    "path_non = test_images_dir+r'\\Non-AMD'\n",
    "\n",
    "\n",
    "path_dest= os.getcwd()+ r'\\preprocess_3_quality\\Baidu_Challenge'\n",
    "path_dest_amd = path_dest+r'\\AMD'\n",
    "path_dest_non = path_dest+r'\\Non-AMD'\n",
    "\n",
    "\n",
    "#prop_quality(path_amd, path_dest_amd )\n",
    "prop_quality(path_non, path_dest_non )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ODIR-5K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_dir = os.getcwd()+ r'\\preprocess_2_crop_resize_align\\ODIR-5k'\n",
    "path_amd = test_images_dir+r'\\AMD'\n",
    "path_non = test_images_dir+r'\\Non-AMD'\n",
    "\n",
    "\n",
    "path_dest= os.getcwd()+ r'\\preprocess_3_quality\\ODIR-5k'\n",
    "path_dest_amd = path_dest+r'\\AMD'\n",
    "path_dest_non = path_dest+r'\\Non-AMD'\n",
    "\n",
    "\n",
    "prop_quality(path_amd, path_dest_amd )\n",
    "prop_quality(path_non, path_dest_non )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RIADD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_dir = os.getcwd()+ r'\\preprocess_2_crop_resize_align\\RIADD'\n",
    "path_amd = test_images_dir+r'\\AMD'\n",
    "path_non = test_images_dir+r'\\Non-AMD'\n",
    "\n",
    "\n",
    "path_dest= os.getcwd()+ r'\\preprocess_3_quality\\RIADD'\n",
    "path_dest_amd = path_dest+r'\\AMD'\n",
    "path_dest_non = path_dest+r'\\Non-AMD'\n",
    "\n",
    "\n",
    "prop_quality(path_amd, path_dest_amd )\n",
    "prop_quality(path_non, path_dest_non )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality assesment to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "        'valid': transforms.Compose([\n",
    "            transforms.Resize(390),\n",
    "            transforms.CenterCrop(256),\n",
    "            transforms.ToTensor()\n",
    "        ]),\n",
    "}\n",
    "\n",
    "image_datasets={}\n",
    "\n",
    "    \n",
    "image_datasets['Baidu-A'] = torchvision.datasets.ImageFolder(root=r'preprocess_3_quality\\Baidu_Challenge\\AMD', transform=data_transforms['valid'])\n",
    "image_datasets['Baidu-N'] = torchvision.datasets.ImageFolder(root=r'preprocess_3_quality\\Baidu_Challenge\\Non-AMD',transform=data_transforms['valid'])\n",
    "image_datasets['Odir-A'] = torchvision.datasets.ImageFolder(root=r'preprocess_3_quality\\ODIR-5K\\AMD', transform=data_transforms['valid'])\n",
    "image_datasets['Odir-N'] = torchvision.datasets.ImageFolder(root=r'preprocess_3_quality\\ODIR-5K\\Non-AMD', transform=data_transforms['valid'])\n",
    "image_datasets['Riadd-A'] = torchvision.datasets.ImageFolder(root=r'preprocess_3_quality\\RIADD\\AMD', transform=data_transforms['valid'])\n",
    "image_datasets['Riadd-N'] = torchvision.datasets.ImageFolder(root=r'preprocess_3_quality\\RIADD\\Non-AMD', transform=data_transforms['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "baidu_idx = int(len(image_datasets['Baidu-A']) *0.28)\n",
    "odir_idx = int(len(image_datasets['Odir-A']) *0.28)\n",
    "riadd_idx = int(len(image_datasets['Riadd-A']) *0.28)\n",
    "\n",
    "count=0\n",
    "\n",
    "for i in range(0,baidu_idx):\n",
    "    img=image_datasets['Baidu-A'][i][0]\n",
    "    save_image(img, 'preprocess_4_create_crop/data/test/AMD/{}.png'.format(count) )\n",
    "    count=count+1\n",
    "\n",
    "for i in range(baidu_idx, len(image_datasets['Baidu-A']) ):\n",
    "    img=image_datasets['Baidu-A'][i][0]\n",
    "    save_image(img, 'preprocess_4_create_crop/data/train/AMD/{}.png'.format(count) )\n",
    "    count=count+1\n",
    "    \n",
    "\n",
    "for i in range(0,odir_idx):\n",
    "    img=image_datasets['Odir-A'][i][0]\n",
    "    save_image(img, 'preprocess_4_create_crop/data/test/AMD/{}.png'.format(count) )\n",
    "    count=count+1\n",
    "\n",
    "for i in range(odir_idx, len(image_datasets['Odir-A']) ):\n",
    "    img=image_datasets['Odir-A'][i][0]\n",
    "    save_image(img, 'preprocess_4_create_crop/data/train/AMD/{}.png'.format(count) )\n",
    "    count=count+1\n",
    "\n",
    "    \n",
    "\n",
    "for i in range(0,riadd_idx):\n",
    "    img=image_datasets['Riadd-A'][i][0]\n",
    "    save_image(img, 'preprocess_4_create_crop/data/test/AMD/{}.png'.format(count) )\n",
    "    count=count+1\n",
    "\n",
    "for i in range(riadd_idx, len(image_datasets['Riadd-A']) ):\n",
    "    img=image_datasets['Riadd-A'][i][0]\n",
    "    save_image(img, 'preprocess_4_create_crop/data/train/AMD/{}.png'.format(count) )\n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test\n",
    "\n",
    "totalFiles=0\n",
    "for base, dirs, files in os.walk(r'preprocess_4_create_crop/data/test/AMD'):\n",
    "    print('Searching in : ',base)\n",
    "    for Files in files:\n",
    "        totalFiles += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "totalFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "count=0\n",
    "\n",
    "for i in range(0,baidu_idx):\n",
    "    img=image_datasets['Baidu-N'][i][0]\n",
    "    save_image(img, 'preprocess_4_create_crop/data/test/Non-AMD/{}.png'.format(count) )\n",
    "    count=count+1\n",
    "\n",
    "for i in range(baidu_idx, len(image_datasets['Baidu-N']) ):\n",
    "    img=image_datasets['Baidu-N'][i][0]\n",
    "    save_image(img, 'preprocess_4_create_crop/data/train/Non-AMD/{}.png'.format(count) )\n",
    "    count=count+1\n",
    "    \n",
    "\n",
    "for i in range(0,odir_idx):\n",
    "    img=image_datasets['Odir-N'][i][0]\n",
    "    save_image(img, 'preprocess_4_create_crop/data/test/Non-AMD/{}.png'.format(count) )\n",
    "    count=count+1\n",
    "\n",
    "for i in range(odir_idx, len(image_datasets['Odir-N']) ):\n",
    "    img=image_datasets['Odir-N'][i][0]\n",
    "    save_image(img, 'preprocess_4_create_crop/data/train/Non-AMD/{}.png'.format(count) )\n",
    "    count=count+1\n",
    "\n",
    "    \n",
    "\n",
    "for i in range(0,riadd_idx):\n",
    "    img=image_datasets['Riadd-N'][i][0]\n",
    "    save_image(img, 'preprocess_4_create_crop/data/test/Non-AMD/{}.png'.format(count) )\n",
    "    count=count+1\n",
    "\n",
    "for i in range(riadd_idx, len(image_datasets['Riadd-N']) ):\n",
    "    img=image_datasets['Riadd-N'][i][0]\n",
    "    save_image(img, 'preprocess_4_create_crop/data/train/Non-AMD/{}.png'.format(count) )\n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Conditional Stylegan Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.getcwd()+r'\\preprocess_4_create_crop\\data\\train'\n",
    "\n",
    "dataset={}\n",
    "dataset['labels'] = []\n",
    "\n",
    "new_name={}\n",
    "\n",
    "i=-2\n",
    "idd=0\n",
    "\n",
    "for dirpath, _, files in os.walk(path):\n",
    "    i=i+1\n",
    "    for file_name in files:\n",
    "        idd= idd+1\n",
    "        new_name[os.path.join(dirpath, file_name)] = str(idd)+'.png'\n",
    "        dataset['labels'].append([str(idd)+'.png',i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "dataset_dir = os.getcwd() + r'\\preprocess_5_stylegan_format\\train'\n",
    "\n",
    "try:\n",
    "    os.mkdir(dataset_dir)\n",
    "except OSError as error:\n",
    "    print(error)\n",
    "    \n",
    "for p in new_name.keys():\n",
    "    shutil.copy2(p, os.path.join( dataset_dir, new_name[p] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dumps(dataset)\n",
    "\n",
    "with open(dataset_dir+r'\\dataset.json', 'w') as outfile:\n",
    "    json.dump(dataset, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
